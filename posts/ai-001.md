---
id: "ai-001"
title: "Bayesian Inference"
date: "2026-1-19"
category: "AI/Fundamental Theory"
tags: ["Probability", "Uncertainty", "Bayesian rule"]
pinned: true
excerpt: "Bayesian Inference 개념."
---

A가 주어졌을 때 B는 무엇인가?

$P(B|A)$

Propositional logic 범위 내에서라면 이 질문에 대해 완전한 확실성을 갖고 답을 하겠지만, 이러한 논리는 불확실성이 존재하는 현실세계를 다루기엔 지나치게 경직되어 있다. 실전 문제에서, 대부분의 경우 종속변수 B와 독립변수 A에 대한 측정값은 본질적으로 노이즈가 있고 부정확하므로 이 둘의 관계는 non-deterministic하다. 따라서 확률론을 통해 불확실성이 존재하는 상황에서 의미있는 추론을 위해 일관된 원칙과 논리가 필요하다. 머신러닝의 주요 과업은 A가 주어졌을 때, 조건부 확률 $P(B|A)$를 통해 B가 참일 가능성을 추론하기 위해 적절하게 P(B|A)를 근사하는 것이다. Bayesian Inference가 전면에 등장하는 것이 이 과정에서이다.

$P(B|A)=f(A:w)$

여기서 w는 모델의 모든 조정 가능한 매개변수 벡터를 나타낸다. 이후 변수들의 N개 예시로 구성된 집합 $D=({A_n}, {B_n})^{N}_{n=1}$이 주어지면, 전통적인 접근 방식은 조정 가능한 매개변수에 대해 D에 대한 모델의 정확도 측정값을 최대화하거나 손실 측정값을 최소화 하는 것을 포함한다. 그 다음 매개변수 w를 최적값을 설정하고 $f(A;w)$를 평가함으로써 주어진 A에 대해 알려지지 않은 B를 예측할 수 있는 것이다. 전통적인 관점에서의 point estimation에서는 $f(A;w)$와 같은 파라미터화 된 모델을 두고 손실을 최소화하는 w를 찾으므로 복잡한 모델 (대부분이 그러하듯) $P(B|A)$의 진짜 분포를 과적합으로 망칠 수 있다. Bayesian Inference의 핵심은 w와 같은 매개변수를 A와 B와 똑같이 random variable로 취급하는 것이다. 따라서, 조건부 확률은 이제 $P(B|A,w)$가 되며, B의 확률이 A 뿐만 아니라 매개변수 설정에 의존한다는 점이 명시적으로 드러난다. 어떤 품질 측정값을 최적화하는 것으로 구성된 학습 대신 bayes rule을 통해 매개변수 w에 대한 분포를 추론한다.

w에 대한 posterior 분포를 얻기 위해서는 데이터를 관찰하기 전에 prior 분포 $P(w)$를 지정해야 한다. Bayesian Inference는 모델링 과정의 모든 불확실성 원인을 통합되고 일관된 방식으로 처리하며, 가정과 제약 조건을 명시하도록 가정한다. 또한 Bayesian 접근 방식은 모든 무관한 변수들을 적분 제거 (integrating out, marginalisation) 함으로써 Ockaham's Razor (불필요하게 복잡한 모델을 자동으로 깎아냄)가 구현된다. 즉, Bayesian 프레임워크 하에서는 불필요한 복잡성 없이 데이터를 충분히 설명하는 단순한 모델을 선호하는 경향이 존재한다.
